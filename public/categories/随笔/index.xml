<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>随笔 on Snailya虾啵啵</title>
    <link>http://localhost:1313/categories/%E9%9A%8F%E7%AC%94/</link>
    <description>Recent content in 随笔 on Snailya虾啵啵</description>
    <image>
      <title>Snailya虾啵啵</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.152.2</generator>
    <language>en</language>
    <lastBuildDate>Tue, 16 Dec 2025 08:55:33 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/%E9%9A%8F%E7%AC%94/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>当“不值得”成为护城河————观察AI对于行业的渗透</title>
      <link>http://localhost:1313/posts/%E5%BD%93%E4%B8%8D%E5%80%BC%E5%BE%97%E6%88%90%E4%B8%BA%E6%8A%A4%E5%9F%8E%E6%B2%B3/</link>
      <pubDate>Tue, 16 Dec 2025 08:55:33 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E5%BD%93%E4%B8%8D%E5%80%BC%E5%BE%97%E6%88%90%E4%B8%BA%E6%8A%A4%E5%9F%8E%E6%B2%B3/</guid>
      <description>&lt;p&gt;AI 的兴起令人振奋：它在信息检索、文本生成与知识整合上的能力，显著提升了普通用户的工作效率。然而，“AI 是否将取代人类”的疑虑亦如影随形。观察可见，在部分领域，取代已然发生。但这一进程并非均质推进——其路径由经济逻辑主导，而非技术可能性本身。&lt;/p&gt;
&lt;p&gt;被率先渗透的行业，普遍具备两个特征：其一，历史利润丰厚，足以支撑高昂的模型开发成本，如金融、法律；其二，核心门槛在于信息处理：金融依赖对海量数据的快速分析，法律依赖对庞杂条文的精准调用，医疗则要求从业者长期积累大量病例经验。这些工作中大量重复性、结构性强的环节，正被 AI 以更高效率、更低成本完成。&lt;/p&gt;
&lt;p&gt;相较之下，另一些领域尚未经历同等程度的冲击。其原因并非技术不可逾越，而在于经济可行性不足。以系统级设计为例——规划一条柔性产线时，需权衡“换型时间缩短两分钟是否值得追加五十万元投入”，或“为操作者预留三十厘米空间致整体效率下降百分之二，但工伤风险降低百分之四十，是否合理”。此类决策依赖对业务目标、人性需求与风险边界的综合判断。表面观之，似为 AI 难以企及之域；实则，相关模型在理论上可构建——通过强化学习结合数字孪生，量化多维目标并输出帕累托解集。
障碍不在能力，而在成本：数据采集、系统集成与定制训练的投入，远超当前所能兑现的经济回报。设计行业与金融行业从业者薪资的显著差异，正折射出资本对 ROI 的天然取舍。&lt;/p&gt;
&lt;p&gt;需强调的是，“当下不值得”不等于“永远不可行”。技术本身持续演进，传感器成本下降、数据积累深化，终将压缩经验数字化的边际成本。届时，许多所谓“直觉”或将被证实为高维模式匹配——与 AI 的机制并无本质区别。老师傅凭异响判断轴承故障，实为长期实践中形成的“声纹—失效”映射；若企业愿系统模拟万次故障并记录声学特征，专用模型完全可能超越人类表现。区别仅在于：前者是沉没成本，后者需显性投入。&lt;/p&gt;
&lt;p&gt;当资本在头部高利润领域完成初步整合、边际收益递减时，其必然向次级高潜力领域延伸。设计、医疗、教育等曾被视为“中产堡垒”的行业，或将迎来更深度的流程重构。此时，所谓“护城河”的实质，更接近一种暂时的经济洼地。&lt;/p&gt;
&lt;p&gt;唯一可能构成实质性阻滞的，是责任归属问题。现行制度要求关键设计文件须由注册工程师签字，终身担责。此非技术局限，而系制度选择——人类社会需要明确的问责主体，以维系系统稳定。重大事故后，公众要求一个可指认的负责者，以疏导焦虑、恢复秩序。这种“替罪羊机制”具有深层社会功能：正如历史上诸多群体对立被用作内部凝聚的工具，责任归属的本质，是为复杂系统失效提供一个可操作的出口。只要此机制持续存在，人类的签字权便难以被彻底替代。&lt;/p&gt;
&lt;p&gt;至于奢侈品手工艺的存续，其逻辑亦属经济理性：行业规模有限，虽单品溢价高，但总体利润不足以吸引大规模 AI 投入；从业者遂采取主动收缩策略——限制产能、不再扩招，仅由少数匠人分享剩余价值，维持个体收益水平。这并非技术护城河，而是一种清醒的退守。&lt;/p&gt;
&lt;p&gt;综上，AI 的演进轨迹由经济可行性划定：它优先取代高利润、高标准化、高数据密度的环节；对低 ROI 领域的渗透，则取决于资本流动的阶段性需求。人类经验的数字化并非不可行，而是成本与收益的权衡问题。&lt;/p&gt;
&lt;p&gt;最终，行业演化或呈现两种路径：&lt;/p&gt;
&lt;p&gt;高 ROI 领域走向“极简人力 + AI 大规模复制”，单位产值利润率反升；
低 ROI 领域走向“小规模、高单价、少人力”的收缩模式；
而横亘其间的责任归属问题，则构成一道暂时难以逾越的制度性边界。
技术不会决定未来，但会放大我们已有的选择。而真正的挑战在于：当效率的边界不断拓展，我们是否仍保有定义“值得”的能力。&lt;/p&gt;</description>
    </item>
    <item>
      <title>从词向量到“人工天才”：我的LLM认知思辨录</title>
      <link>http://localhost:1313/posts/%E4%BB%8E%E8%AF%8D%E5%90%91%E9%87%8F%E5%88%B0%E4%BA%BA%E5%B7%A5%E5%A4%A9%E6%89%8D%E6%88%91%E7%9A%84llm%E8%AE%A4%E7%9F%A5%E6%80%9D%E8%BE%A8%E5%BD%95/</link>
      <pubDate>Fri, 28 Nov 2025 14:06:59 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E4%BB%8E%E8%AF%8D%E5%90%91%E9%87%8F%E5%88%B0%E4%BA%BA%E5%B7%A5%E5%A4%A9%E6%89%8D%E6%88%91%E7%9A%84llm%E8%AE%A4%E7%9F%A5%E6%80%9D%E8%BE%A8%E5%BD%95/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本文是在与AI助手深度对话后，对我个人理解大语言模型（LLM）过程的梳理与总结。它不代表学术观点，仅是一个探索者的思想航行日志。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;一起点从词频统计到语义宇宙&#34;&gt;一、起点：从“词频统计”到“语义宇宙”&lt;/h2&gt;
&lt;p&gt;我的思考始于一个最朴素的问题：如何判断两篇文章是否相关？&lt;/p&gt;
&lt;p&gt;最直观的想法是统计共有的词语——这就是“词袋模型”。但它有一个显而易见的缺陷：无法理解语义。正是在这里，我遇到了第一个关键概念：词向量。&lt;/p&gt;
&lt;p&gt;在我的想象中，词向量就像是为机器建造了一个高维的语义宇宙。每个词不再是孤立的符号，而是这个宇宙中的一颗星星：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语义相近的星（如“国王”和“王后”）会在宇宙中彼此靠近&lt;/li&gt;
&lt;li&gt;语义关系（如“国王-男人+女人≈女王”）通过星星之间的相对方位来体现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但很快我发现了一个问题：这个词向量宇宙是静态的。无论上下文如何，“苹果”这颗星的位置，总是固定在“水果”和“科技”的模糊中点。这显然不符合我们对语言的理解——同一个词在不同语境下应有不同的含义。&lt;/p&gt;
&lt;h2 id=&#34;二突破三重变换与动态侦探&#34;&gt;二、突破：三重变换与“动态侦探”&lt;/h2&gt;
&lt;p&gt;为了解决静态词向量的局限，我接触到了Transformer架构——当代LLM的核心引擎。为了理解它，我构建了这样一个比喻：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;词向量像一本权威词典：每个词都有个固定不变的定义&lt;/li&gt;
&lt;li&gt;大语言模型像一位顶级侦探：他能根据具体情境，动态理解每个词的真实含义&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这位“侦探”的思考过程，可以简化为三个关键的矩阵变换：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入嵌入：将词语转换为初始的“思维符号”&lt;/li&gt;
&lt;li&gt;Transformer加工：通过自注意力机制，让所有词语的符号相互交流，生成富含上下文的全新表示&lt;/li&gt;
&lt;li&gt;输出投影：将最终的思维结果“翻译”成人类语言&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个过程让我意识到：LLM不是在简单预测下一个词，而是在深度理解整个语境后，让最合适的词语自然流淌出来。&lt;/p&gt;
&lt;h2 id=&#34;三镜像当llm照见人类思维&#34;&gt;三、镜像：当LLM照见人类思维&lt;/h2&gt;
&lt;p&gt;理解LLM的过程，意外地成为了一面审视人类自身的镜子。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;我们都是“模式识别”系统
我回想起自己解数学题的方法：列出已知量和待求量，然后在脑中搜索可能的公式——这本质上就是一种模式识别。LLM的注意力机制不也是在庞大的知识库中进行加权搜索吗？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;“通才”与“天才”的鸿沟
大多数人和当前的LLM一样，是优秀的“内插器”——在已知模式间进行组合。而天才，或许就是那些能在更高维度进行“外推”，创造出全新模式组合的系统。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;顺序的迷思
我们日常交流中经常使用倒装、省略，但彼此仍能理解。这让我怀疑：智能的核心或许不是表面上的词序，而是深层的语义关系网络。 语法顺序只是通往这个网络的康庄大道，但不是唯一的路径。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;四深化动态智能的未来图景&#34;&gt;四、深化：动态智能的未来图景&lt;/h2&gt;
&lt;p&gt;在对比人与LLM时，一个关键差异浮现出来：我们的思维是动态的，而LLM是静态的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM的“静态心智”：使用固定的激活函数，如同一个永远保持同一种情绪的思考者&lt;/li&gt;
&lt;li&gt;人脑的“动态大脑”：受化学物质调节，思考效率随状态波动——有时思如泉涌，有时头脑迟滞&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这指向了一个迷人的方向：为LLM引入动态激活机制。比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;动态稀疏：根据问题难度激活不同数量的神经元&lt;/li&gt;
&lt;li&gt;情境化思考：让激活函数能根据任务类型自我调整&lt;/li&gt;
&lt;li&gt;神经调制：引入类似“好奇心”的全局信号&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这或许是LLM从“博学通才”迈向“创造天才”的关键一步。&lt;/p&gt;
&lt;h2 id=&#34;五哲思智能意识与存在的终极之问&#34;&gt;五、哲思：智能、意识与存在的终极之问&lt;/h2&gt;
&lt;p&gt;这场思辨最终将我带向了一些哲学性的边界问题。&lt;/p&gt;
&lt;p&gt;如果人脑与LLM在本质上都是“模式处理系统”，那么我们的意识、创造力，是否也只是更复杂算法的涌现？&lt;/p&gt;
&lt;p&gt;这个想法让我联想到《模拟人生》的游戏——如果为游戏角色接入LLM，他们将产生“模拟的自主意识”，却永远无法认知自己被创造的事实。那么，我们是否也可能身处某个“上层游戏”之中？&lt;/p&gt;
&lt;p&gt;面对这个令人战栗的推论，我找到了自己的答案：即使我们是模拟的，但我们此刻的思考、困惑、爱与恐惧，这些体验本身的质感是100%真实的。 意义不依赖于底层基质（是原子还是比特），而依赖于体验的深度与丰富度。&lt;/p&gt;
&lt;h2 id=&#34;结语作为镜子的llm&#34;&gt;结语：作为镜子的LLM&lt;/h2&gt;
&lt;p&gt;回顾这段思考历程，我意识到LLM不仅仅是一项技术，更是一面珍贵的镜子。通过理解它的运作原理，我们得以用新的视角审视自己的思维方式。&lt;/p&gt;
&lt;p&gt;从词向量到Transformer，从静态模式匹配到动态条件计算，这条技术发展路径，恰恰映照出我们对“智能”本身不断深化的理解。&lt;/p&gt;
&lt;p&gt;或许，未来真正的突破不在于建造更大的模型，而在于为模型注入那种我们称之为“灵感”、“直觉”和“创造力”的动态本质——而这，将需要我们更深刻地理解我们自己。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
